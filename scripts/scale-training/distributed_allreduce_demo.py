#!/usr/bin/env python3
"""
distributed_comm_test.py

åŠŸèƒ½ï¼šåˆ†å¸ƒå¼é€šä¿¡æµ‹è¯•è„šæœ¬ï¼ˆæ”¯æŒ NPU/GPUï¼‰
ç”¨é€”ï¼šéªŒè¯ HCCL/NCCL é€šä¿¡æ˜¯å¦æ­£å¸¸
è¿è¡Œï¼štorchrun --nproc-per-node=8 run_single_node.sh ...
"""

import os
import sys
from datetime import timedelta

import torch
import torch.distributed as dist

# å°è¯•å¯¼å…¥ NPU æ”¯æŒï¼ˆAscend ç¯å¢ƒï¼‰
try:
    import torch_npu
    from torch_npu.contrib import transfer_to_npu  # noqa: F401
    HAS_NPU = True
except ImportError:
    HAS_NPU = False


def get_device(local_rank: int) -> torch.device:
    """
    æ ¹æ®å¯ç”¨è®¾å¤‡è¿”å›æ­£ç¡®çš„ torch.deviceã€‚
    ä¼˜å…ˆä½¿ç”¨ NPUï¼ˆAscendï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ CUDAã€‚
    """
    if HAS_NPU:
        assert hasattr(torch, 'npu'), 'torch_npu æ‰©å±•æœªæ­£ç¡®åŠ è½½'
        return torch.device(f'npu:{local_rank}')
    else:
        return torch.device(f'cuda:{local_rank}')


def set_device(local_rank: int):
    """ä¸ºå½“å‰è¿›ç¨‹è®¾ç½®é»˜è®¤è®¾å¤‡"""
    if HAS_NPU:
        torch.npu.set_device(local_rank)  # âœ… æ­£ç¡®æ–¹å¼
    else:
        torch.cuda.set_device(local_rank)


def get_backend() -> str:
    """æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„åˆ†å¸ƒå¼åç«¯"""
    if HAS_NPU:
        return 'nccl'
    else:
        return 'nccl'


# ========================================
# æ—¥å¿—ä¸è¾…åŠ©å‡½æ•°
# ========================================


def log_rank(msg: str, rank=None):
    """å¸¦ Rank å‰ç¼€çš„æ—¥å¿—è¾“å‡º"""
    rank_str = f'{rank:2d}' if rank is not None else os.environ.get(
        'RANK', 'unknown')
    print(f'[Rank {rank_str}] {msg}', flush=True)


def validate_tensor(tensor: torch.Tensor, expected: float, op_name: str):
    """éªŒè¯é€šä¿¡ç»“æœ"""
    if abs(tensor.item() - expected) < 1e-5:
        log_rank(f'âœ… {op_name} æˆåŠŸï¼šç»“æœæ­£ç¡®ã€‚', tensor.device.index)
    else:
        log_rank(f'âŒ {op_name} å¤±è´¥ï¼šæœŸæœ› {expected:.1f}ï¼Œå®é™… {tensor.item():.1f}',
                 tensor.device.index)


# ========================================
# åˆ†å¸ƒå¼æ ¸å¿ƒé€»è¾‘
# ========================================


def run(rank: int, world_size: int, local_rank: int):
    """åˆ†å¸ƒå¼è®­ç»ƒæ ¸å¿ƒé€»è¾‘"""
    device = get_device(local_rank)

    # æµ‹è¯• 1: AllReduce (Sum)
    tensor = torch.tensor([float(rank)], dtype=torch.float32, device=device)
    log_rank(
        f'Local Rank {local_rank} | Device: {device} | åˆå§‹å€¼: {tensor.item():.1f}',
        rank)

    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    log_rank(f'AllReduce ç»“æœ: {tensor.item():.1f}', rank)
    expected = world_size * (world_size - 1) / 2
    validate_tensor(tensor, expected, 'AllReduce')
    dist.barrier()

    # æµ‹è¯• 2: Broadcast (rank 0 å‘é€)
    if rank == 0:
        broadcast_tensor = torch.tensor([123.0], device=device)
    else:
        broadcast_tensor = torch.zeros(1, device=device)

    dist.broadcast(broadcast_tensor, src=0)
    log_rank(f'Broadcast åå€¼: {broadcast_tensor.item():.1f}', rank)
    validate_tensor(broadcast_tensor, 123.0, 'Broadcast')
    dist.barrier()

    # æµ‹è¯• 3: AllGather
    gather_list = [
        torch.zeros(1, dtype=torch.float32, device=device)
        for _ in range(world_size)
    ]
    dist.all_gather(gather_list, tensor)
    gathered_values = [
        gathered_tensor.item() for gathered_tensor in gather_list
    ]
    log_rank(f'AllGather ç»“æœ: {gathered_values}', rank)
    validate_tensor(gather_list[0], sum(range(world_size)),
                    'AllGather')  # éªŒè¯ç¬¬ä¸€ä¸ªå…ƒç´ 
    dist.barrier()

    # æµ‹è¯• 4: ReduceScatter
    scatter_list = [
        torch.tensor([float(rank)], dtype=torch.float32, device=device)
        for _ in range(world_size)
    ]
    output = torch.zeros(1, dtype=torch.float32, device=device)
    dist.reduce_scatter(output, scatter_list, op=dist.ReduceOp.SUM)
    log_rank(f'ReduceScatter ç»“æœ: {output.item():.1f}', rank)
    validate_tensor(output, sum(range(world_size)), 'ReduceScatter')

    log_rank('ğŸ‰ æ‰€æœ‰é€šä¿¡æµ‹è¯•å®Œæˆï¼', rank)


def setup_distributed() -> tuple[int, int, int]:
    env = os.environ
    try:
        rank = int(env['RANK'])
        local_rank = int(env.get('LOCAL_RANK', rank))
        world_size = int(env['WORLD_SIZE'])
    except KeyError as e:
        raise EnvironmentError(f"ç¼ºå°‘ç¯å¢ƒå˜é‡: {e}. è¯·ä½¿ç”¨ 'torchrun' å¯åŠ¨ã€‚") from e
    except ValueError as e:
        raise ValueError(f'ç¯å¢ƒå˜é‡æ ¼å¼é”™è¯¯: {e}') from e

    backend = get_backend()
    master_addr = env.get('MASTER_ADDR', 'unknown')
    master_port = env.get('MASTER_PORT', 'unknown')

    log_rank(
        f"æ­£åœ¨åˆå§‹åŒ–è¿›ç¨‹ç»„ backend='{backend.upper()}' "
        f'MASTER={master_addr}:{master_port}', rank)

    dist.init_process_group(backend=backend,
                            rank=rank,
                            world_size=world_size,
                            timeout=timedelta(seconds=60))

    # âœ… æ­£ç¡®è®¾ç½®è®¾å¤‡
    set_device(local_rank)

    if dist.is_initialized():
        log_rank(
            f'åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆã€‚World Size={world_size}, Local Rank={local_rank}',
            rank)
        dist.barrier()

    return rank, world_size, local_rank


def cleanup():
    """å®‰å…¨æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.barrier()  # ç¡®ä¿æ‰€æœ‰ rank éƒ½åˆ°è¾¾æ­¤å¤„
        dist.destroy_process_group()


if __name__ == '__main__':
    try:
        rank, world_size, local_rank = setup_distributed()
        run(rank, world_size, local_rank)
    except KeyboardInterrupt:
        print(f"[Rank {os.environ.get('RANK', 'unknown')}] è¢«ç”¨æˆ·ä¸­æ–­ã€‚",
              file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        rank_str = os.environ.get('RANK', 'unknown')
        print(f'[Rank {rank_str}] å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {type(e).__name__}: {e}',
              file=sys.stderr)
        raise  # ä¿ç•™åŸå§‹ traceback
    finally:
        cleanup()
